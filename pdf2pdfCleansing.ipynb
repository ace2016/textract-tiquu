{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4609d416-9f75-414b-9fb5-fbe30d70eca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pdfplumber\n",
      "  Downloading pdfplumber-0.11.4-py3-none-any.whl.metadata (41 kB)\n",
      "     ---------------------------------------- 0.0/42.0 kB ? eta -:--:--\n",
      "     --------- ------------------------------ 10.2/42.0 kB ? eta -:--:--\n",
      "     --------------------------- ---------- 30.7/42.0 kB 330.3 kB/s eta 0:00:01\n",
      "     -------------------------------------- 42.0/42.0 kB 407.1 kB/s eta 0:00:00\n",
      "Collecting pdfminer.six==20231228 (from pdfplumber)\n",
      "  Downloading pdfminer.six-20231228-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: Pillow>=9.1 in c:\\users\\imman\\miniforge3\\lib\\site-packages (from pdfplumber) (10.4.0)\n",
      "Collecting pypdfium2>=4.18.0 (from pdfplumber)\n",
      "  Downloading pypdfium2-4.30.0-py3-none-win_amd64.whl.metadata (48 kB)\n",
      "     ---------------------------------------- 0.0/48.5 kB ? eta -:--:--\n",
      "     ---------------------------------------- 48.5/48.5 kB 2.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in c:\\users\\imman\\miniforge3\\lib\\site-packages (from pdfminer.six==20231228->pdfplumber) (3.3.2)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in c:\\users\\imman\\miniforge3\\lib\\site-packages (from pdfminer.six==20231228->pdfplumber) (43.0.0)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\users\\imman\\miniforge3\\lib\\site-packages (from cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (1.16.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\imman\\miniforge3\\lib\\site-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (2.22)\n",
      "Downloading pdfplumber-0.11.4-py3-none-any.whl (59 kB)\n",
      "   ---------------------------------------- 0.0/59.2 kB ? eta -:--:--\n",
      "   ---------------------------------------- 59.2/59.2 kB 3.3 MB/s eta 0:00:00\n",
      "Downloading pdfminer.six-20231228-py3-none-any.whl (5.6 MB)\n",
      "   ---------------------------------------- 0.0/5.6 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.5/5.6 MB 14.2 MB/s eta 0:00:01\n",
      "   ------- -------------------------------- 1.0/5.6 MB 13.3 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 1.6/5.6 MB 12.7 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 2.2/5.6 MB 12.9 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 2.9/5.6 MB 13.1 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 3.5/5.6 MB 13.1 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 4.0/5.6 MB 12.8 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 4.7/5.6 MB 13.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 5.4/5.6 MB 13.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  5.6/5.6 MB 13.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 5.6/5.6 MB 11.9 MB/s eta 0:00:00\n",
      "Downloading pypdfium2-4.30.0-py3-none-win_amd64.whl (2.9 MB)\n",
      "   ---------------------------------------- 0.0/2.9 MB ? eta -:--:--\n",
      "   -------- ------------------------------- 0.6/2.9 MB 19.2 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 1.2/2.9 MB 25.1 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 1.3/2.9 MB 11.9 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 2.7/2.9 MB 16.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.9/2.9 MB 14.2 MB/s eta 0:00:00\n",
      "Installing collected packages: pypdfium2, pdfminer.six, pdfplumber\n",
      "  Attempting uninstall: pdfminer.six\n",
      "    Found existing installation: pdfminer.six 20240706\n",
      "    Uninstalling pdfminer.six-20240706:\n",
      "      Successfully uninstalled pdfminer.six-20240706\n",
      "Successfully installed pdfminer.six-20231228 pdfplumber-0.11.4 pypdfium2-4.30.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pdfplumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9db0526b-2107-45ff-8be0-c5ae8f3bbfeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "# Function to clean the text\n",
    "def cleanse_text(text):\n",
    "    unwanted_sections = [\"references\"]\n",
    "    for section in unwanted_sections:\n",
    "        pattern = r'(?i)\\b{}\\b.*?(?=\\n\\n|\\Z)'.format(section)  # Case-insensitive section removal\n",
    "        text = re.sub(pattern, '', text, flags=re.DOTALL)\n",
    "\n",
    "    # Remove references to figures, tables, and page numbers\n",
    "    text = re.sub(r'Figure\\s*\\d+', '', text)  # Remove \"Figure X\" references\n",
    "    text = re.sub(r'Page\\s*\\d+\\s*(of\\s*\\d+)?', '', text)  # Remove \"Page X of Y\" references\n",
    "    text = re.sub(r'Table\\s*\\d+', '', text)  # Remove \"Table X\" references\n",
    "    \n",
    "    # You can add additional patterns to remove any other sections you don't want\n",
    "    text = re.sub(r'Gates Open Research\\s*\\d{4},.*?Last updated:.*?\\n', '', text)  # Remove repeated journal info\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Replace multiple spaces/newlines with a single space\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)  # Remove non-ASCII characters\n",
    "    text = remove_leading_zeros(text)\n",
    "    text = text.strip()\n",
    "    text = re.sub(r'\\. \\.', '.', text)  # Fix any erroneous spaces between periods\n",
    "\n",
    "    # Ensure paragraphs are maintained\n",
    "    text = re.sub(r'\\n\\s*\\n', '\\n\\n', text)  # Ensure paragraph breaks are preserved\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "# Function to remove leading zeros from numbers\n",
    "def remove_leading_zeros(text):\n",
    "    pattern = r'\\b0+(\\d+(\\.\\d+)?)\\b'\n",
    "    result = re.sub(pattern, r'\\1', text)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8d6f5434-16e5-4394-a232-6f3f06b595b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to fix encoding issues\n",
    "def fix_encoding_issues(text):\n",
    "    try:\n",
    "        text = text.encode('latin1').decode('utf-8')\n",
    "    except UnicodeEncodeError:\n",
    "        pass  # If encoding fails, just return the original text\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "82955614-df9a-4c87-b272-6c3d82c65289",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract the title if it's the first sentence before author names\n",
    "\n",
    "def extract_title(text):\n",
    "    # This regex will capture all text before the \"[version\" part\n",
    "    title_match = re.search(r'^(.+?)\\s*\\[version.*$', text, re.DOTALL)\n",
    "    if title_match:\n",
    "        return title_match.group(1).strip()\n",
    "    return None\n",
    "\n",
    "\n",
    "# Function to extract DOI\n",
    "def extract_doi(text):\n",
    "    # Fix encoding issues\n",
    "    text = fix_encoding_issues(text)\n",
    "    \n",
    "    # Regex pattern to match a DOI\n",
    "    match = re.search(r'10\\.\\d{4,9}/[-._;()/:A-Z0-9]+', text, re.IGNORECASE)\n",
    "    \n",
    "    if match:\n",
    "        doi = match.group(0).strip()\n",
    "        \n",
    "        # Clean up any trailing or leading non-DOI characters\n",
    "        doi = re.sub(r'[^a-zA-Z0-9./:-]+$', '', doi)\n",
    "        \n",
    "        return doi\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "# Function to extract authors\n",
    "def extract_authors(text):\n",
    "    text = fix_encoding_issues(text)\n",
    "    # This regex looks for a list of names separated by commas\n",
    "    match = re.search(r'(?m)^\\s*[A-Z][a-z]+(?: [A-Z]\\.)?(?:, [A-Z][a-z]+(?: [A-Z]\\.)?)*', text)\n",
    "    return match.group(0).strip() if match else \"\"\n",
    "\n",
    "# Function to extract FullTextURL\n",
    "def extract_fulltexturl(text):\n",
    "    text = fix_encoding_issues(text)  # Fix any encoding issues first\n",
    "    \n",
    "    # Use regex to search for a line that contains 'FulltextUrl:' followed by a valid URL\n",
    "    match = re.search(r'https?://[^\\s]+', text)\n",
    "    \n",
    "    if match:\n",
    "        url = match.group(0).strip()\n",
    "        \n",
    "        # Remove any trailing non-URL characters (e.g., 'List')\n",
    "        url = re.sub(r'[^\\w:/?=&.-]+$', '', url)  # Remove trailing non-URL characters\n",
    "        \n",
    "        return url\n",
    "    else:\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6d2b3f02-cf57-4368-940a-0423cc687b06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Open Research Europe\n",
      "Open Research Europe 2024, 4:133 Last updated: 05 JUL 2024\n",
      "RESEARCH ARTICLE\n",
      "Enriching Earth observation datasets through semantics for\n",
      "climate change applications: The EIFFEL ontology\n",
      "DOI: 10.12688/openreseurope.17992.1\n",
      "FullTextURL: https://doi.org/10.12688/openreseurope.17992.1\n",
      "\n",
      "FullTextContent (first 1000 characters):\n",
      " Open Research Europe Open Research Europe 2024, 4:133 Last updated: 5 JUL 2024 RESEARCH ARTICLE Enriching Earth observation datasets through semantics for climate change applications: The EIFFEL ontology[version 1; peer review: awaiting peer review] Benjamin Molina 1, Carlos E. Palau1, Jaime Calvo-Gallego 2 1Communication Department, Universitat Politecnica de Valencia, Cam  de Vera, s/n, Valencia, 46022, Spain 2Computing and Automatics Department, Campus Viriato,scuela Polit cnica Superior de Zamora, Avenida de Requejo, 33,, Universidad de Salamanca, Zamora, 49022, Spain v1 First published: 2 Jul 2024, 4:133 Open Peer Review https://doi.org/10.12688/openreseurope.17992.1 Latest published: 2 Jul 2024, 4:133 https://doi.org/10.12688/openreseurope.17992.1 Approval Status AWAITING PEER REVIEW Any reports and responses or comments on the Abstract article can be found at the end of the article. Background Earth Observation (EO) datasets have become vital for decision support applications, particularly from open satellite portals that provide extensive historical datasets. These datasets can be integrated with in-situ data to power artificial intelligence mechanisms for accurate forecasting and trend analysis. However, researchers and data scientists face challenges in finding appropriate EO datasets due to inconsistent metadata structures and varied keyword descriptions. This misalignment hinders the discoverability and usability of EO data. Methods To address this challenge, the EIFFEL ontology (EIFF-O) is proposed. EIFF-O introduces taxonomies and ontologies to provide (i) global classification of EO data and (ii) linkage between different datasets through common concepts. The taxonomies specified by the European Association of Remote Sensing Companies (EARSC) have been formalized and implemented in EIFF-O. Additionally, EIFF-O incorporates: 1. An Essential Climate Variable (ECV) ontology, defined by the Global Climate Observing System (GCOS), is embedded and tailored for Climate Change (CC) applications. 2. Open Research Europe Open Research Europe 2024, 4:133 Last updated: 5 JUL 2024 The Sustainable Development Goals (SDG) ontology is included to facilitate linking datasets to specific targets. 3. The ontology extends schema.org vocabularies and promotes the use of JavaScript Object Notation for Linked Data (JSON-LD) formats for semantic web integration. Results EIFF-O provides a unified framework that enhances the discoverability, usability, and application of EO datasets. The implementation of EIFF-O allows data providers and users to bridge the gap between varied metadata descriptions and structured classification, thereby facilitating better linkage and integration of EO datasets. Conclusions The EIFFEL ontology represents a significant advancement in the organization and application of EO datasets. By embedding ECV and SDG ontologies and leveraging semantic web technologies, EIFF-O not only streamlines the data discovery process but also supports diverse applications, particularly in Climate Change monitoring and Sustainable Development Goals achievement. The open-source nature of the ontology and its associated tools promotes rapid adoption among developers Plain language summary Satellites and other tools used to observe Earth provide a lot of data that can help us make decisions, like predicting the weather or understanding climate change. However, these large collections of data are often disorganized and described differently by different people, which makes it hard for scientists and researchers to find and use the information they need. To solve this problem, the paper introduces the EIFFEL Ontology (EIFF- O). This is a new system designed to organize and link Earth observation data in a way that makes it easier to find and use. It does this by creating common categories and connections between different data sets. Three important features of EIFF-O are: It focuses on climate change, helping to categorize important climate data. It connects the data to global goals for sustainable development, which helps in achieving specific environmental targets. Open Research Europe Open Research Europe 2024, 4:133 Last updated: 5 JUL 2024 It links these data sets to the wider internet in a way that makes them easier to share and understand. The EIFF-O system is freely available for anyone to use and comes with tools that help developers quickly implement it in their projects. This makes it easier for everyone to access and benefit from Earth observation data. Keywords Ontology and semantics, Climate change mitigation and adaptation, Earth Observation (EO), Essential Climate Variable, Sustainable Development Goals, EO taxonomy This article is included in the Horizon 2020 gateway. Corresponding author: Benjamin Molina (benmomo@upvnet.upv.es) Author roles: Molina B: Conceptualization, Methodology, Software, Validation, Visualization, Writing   Original Draft Preparation, Writing   Review & Editing; Palau CE: Conceptualization, Funding Acquisition, Investigation, Project Administration, Resources, Supervision, Writing   Review & Editing; Calvo-Gallego J: Conceptualization, Investigation, Resources, Writing   Review & Editing Competing interests: No competing interests were disclosed. Grant information: This work was funded by the European Commission under project EIFFEL (Revealing the Role of GEOSS as the Default Digital Portal for Building Climate Change Adaptation Mitigation Applications) and Grant Agreement 101003518. This research was supported by the Spanish Agencia Estatal de Investigaci n under Grant Number PID2021-126483OB-I00 and the Universidad de Salamanca Research Program under Grant Number PIC2-2021-2 The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript. Copyright:   2024 Molina B et al. This is an open access article distributed under the terms of the Creative Commons Attribution License , which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited. How to cite this article: Molina B, Palau CE and Calvo-Gallego J. Enriching Earth observation datasets through semantics for climate change applications: The EIFFEL ontology [version 1; peer review: awaiting peer review] Open Research Europe 2024, 4:133 https://doi.org/10.12688/openreseurope.17992.1 First published: 2 Jul 2024, 4:133 https://doi.org/10.12688/openreseurope.17992.1 Open Research Europe 2024, 4:133 Last updated: 5 JUL 2024 1. Introduction to work on highlighting semantic versus semantic interoper- The Earth Observation (EO) domain is expected to grow ability and studying various taxonomies and thesauri, but none increasingly in the upcoming years, from  2.8 billion in 2021 of them focused on EO applications at that time4. Sadly, the to  5.5 billion in 20311; not only are satellite services offer- work was discontinued, and no output ontology was provided ing vast amounts of real-time and historical data but they or included in the system. are steadily being linked with in-situ data in local areas. Combining local in-situ data with satellite data is difficult The architecture of GEOSS has experienced significant because the observed environment differs significantly; however, changes during the last decade, and some categories have been the expected benefits derived from their synergies in terms of added; however, no clear taxonomy could be found. Searches validation and/or accuracy (spatial and temporal) make it are still syntactic and use various protocols. OpenSearch5 is worthwhile to promote interoperability and discoverability the most common interface used by the Data Access Broker between them. (DAB), through which most data providers integrate with GEOSS. Although there are some guidelines for this broker inte- Therefore, the management of the related metadata is essential gration process, there is no Linked Data approach, where EO to promote this integration, as well as the rapid development datasets can be semantically published and easily harvested of decision-making applications, which base their forecasts by search engines. and recommendations on the information provided by local and satellite datasets. In addition to short-term weather In contrast, Copernicus provides services in six main areas: forecasts, the climate change domain for adaptation and Copernicus Atmosphere Monitoring Service (CAMS), Coper- mitigation policies is most probably one of the top environ- nicus Marine Environment Monitoring Service (CMEMS), ments under study, as there is a need to consider global and Copernicus Land Monitoring Service (CLMS), Copernicus historical data, even if policies are taken only locally (think Climate Change Service (C3S), Copernicus Emergency Manage- global, act local). ment Service (CEMS), and security (e.g., border surveillance). As this study focuses on Climate Change, C3S is the most important Data can be global and local but private or open. This study service under study. Among the different aspects of C3S, focuses only on open data and publicly available portals to and from the perspective of building an ontology, the Com- access the EO datasets. Private data are not considered, but two mon Data Model (CDM)6 is probably the closest approach; comments are worth mentioning. First, many local decision- it provides a homogeneous format for all data and products makers using open satellite data already own or have access in the Climate Data Store (CDS) so that they can be properly to some sort of (private) local sensor network; thus, they per- processed in the associated toolkit. The CDM is based on the Cli- form integration internally in their own way. Second, some mate and Forecast (CF)7 convention, but it is mainly restricted large private data providers are noticing the huge storage capac- to internal toolkit usage. To the authors knowledge, it is not ity required for historical data; consi\n"
     ]
    }
   ],
   "source": [
    "import pdfplumber\n",
    "\n",
    "# Load the PDF file\n",
    "# pdf_path = 'climate.pdf'\n",
    "pdf_path = 'sample_eu.pdf'\n",
    "content = \"\"\n",
    "\n",
    "# Extract text from each page in the PDF\n",
    "with pdfplumber.open(pdf_path) as pdf:\n",
    "    for page in pdf.pages:\n",
    "        content += page.extract_text() + \" \"\n",
    "\n",
    "# Extract the required fields using the provided functions\n",
    "title = extract_title(content)\n",
    "doi = extract_doi(content)\n",
    "full_text_url = extract_fulltexturl(content)\n",
    "cleaned_content = cleanse_text(content)\n",
    "\n",
    "# Display the extracted information\n",
    "extracted_data = {\n",
    "    \"Title\": title,\n",
    "    \"DOI\": doi,\n",
    "    \"FullTextURL\": full_text_url,\n",
    "    \"FullTextContent\": cleaned_content\n",
    "}\n",
    "\n",
    "# Printing the results\n",
    "print(\"Title:\", extracted_data[\"Title\"])\n",
    "print(\"DOI:\", extracted_data[\"DOI\"])\n",
    "print(\"FullTextURL:\", extracted_data[\"FullTextURL\"])\n",
    "print(\"\\nFullTextContent (first 1000 characters):\\n\", extracted_data[\"FullTextContent\"][:10000])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "33ac964d-ddc4-4cb1-9705-1ba5602be420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF saved successfully to: extracted_data.pdf\n"
     ]
    }
   ],
   "source": [
    "from fpdf import FPDF\n",
    "\n",
    "# Initialize PDF\n",
    "pdf = FPDF()\n",
    "pdf.set_auto_page_break(auto=True, margin=15)\n",
    "pdf.add_page()\n",
    "\n",
    "# Set title, if available\n",
    "pdf.set_font(\"Arial\", 'B', 16)\n",
    "title = extracted_data.get(\"Title\") or \"No Title Available\"\n",
    "pdf.multi_cell(0, 10, title, align='C')\n",
    "pdf.ln(10)  # Add space after title\n",
    "\n",
    "# Set DOI and FullTextURL, if available\n",
    "pdf.set_font(\"Arial\", 'B', 12)\n",
    "doi = extracted_data.get(\"DOI\") or \"DOI not available\"\n",
    "full_text_url = extracted_data.get(\"FullTextURL\") or \"Full Text URL not available\"\n",
    "pdf.cell(0, 10, f\"DOI: {doi}\", ln=True)\n",
    "pdf.cell(0, 10, f\"FullTextURL: {full_text_url}\", ln=True)\n",
    "pdf.ln(10)  # Add space before content\n",
    "\n",
    "# Set the content with normal font and handle None type\n",
    "pdf.set_font(\"Arial\", '', 12)\n",
    "full_text_content = extracted_data.get(\"FullTextContent\") or \"No content available\"\n",
    "pdf.multi_cell(0, 10, full_text_content)\n",
    "\n",
    "# Save the PDF file\n",
    "pdf_file_path = \"extracted_data.pdf\"\n",
    "pdf.output(pdf_file_path)\n",
    "print(f\"PDF saved successfully to: {pdf_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0589bf-1ee5-4909-9585-89b45fbfed30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00edfc95-12af-4cbb-943f-05bfb3793f06",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
